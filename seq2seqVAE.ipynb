{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from os import path as osp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = {\"batch-size\":128, \"epochs\": 10, \"no-cuda\": False, \"seed\":1, \"log_interval\": 10, \"hidden_size\":20, \"intermediate_size\":128, \"widen-factor\":1}\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentsDataset(Dataset):\n",
    "    def __init__(self, list_IDs, labels, mode):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        \n",
    "        # Load data and get label\n",
    "        if mode == 'train':\n",
    "            X = torch.load('/hdd/datasets/Moments_in_Time_Mini/training-sound/' + labels[ID] + '/' + ID + '.wav')\n",
    "        elif mode == 'test':\n",
    "            X = torch.load('/hdd/datasets/Moments_in_Time_Mini/validation-sound/' + labels[ID] + '/' + ID + '.wav')\n",
    "        y = self.labels[ID]\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "        #print('---- df loc-----')\n",
    "        #print(self.df.iloc[idx])\n",
    "        #print('idx', idx)\n",
    "        \n",
    "        #print('file loc', file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paremeters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 100\n",
    "\n",
    "# Dictionaries\n",
    "#create dictionaries for the training and validation set, which return the IDs of the data\n",
    "    #partition['training'] = {ID1, ID2, ....}\n",
    "    #partition[validation] = {ID1, ID2, ....}\n",
    "#create a dictionary called labels where, for each ID of the dataset, the associated label is labels[ID]\n",
    "partition = {}\n",
    "partition['train'] = []\n",
    "partition['validation'] = []\n",
    "labels = {}\n",
    "\n",
    "#iterate through training-sound/file_list.txt\n",
    "with open(\"/hdd/datasets/Moments_in_Time_Mini/training-sound/file_list.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        #split line by '/' to get the action and the file name\n",
    "        split_by_action = line.split(\"/\")\n",
    "        #delimit directory name from string\n",
    "        action = split_by_action[0]\n",
    "        split_by_filename = split_by_action[1].split(\".\")\n",
    "        filename = split_by_filename[0]\n",
    "        \n",
    "        if os.path.isfile(\"/hdd/datasets/Moments_in_Time_Mini/training-sound/\" + action + \"/\" + split_by_action[1]):\n",
    "            partition['train'].append(filename)\n",
    "            labels[filename] = action\n",
    "      \n",
    "#iterate through validation-sound/file_list.txt\n",
    "with open(\"/hdd/datasets/Moments_in_Time_Mini/validation-sound/file_list.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        #split line by '/' to get the action and the file name\n",
    "        split_by_action = line.split(\"/\")\n",
    "        #delimit directory name from string\n",
    "        action = split_by_action[0]\n",
    "        split_by_filename = split_by_action[1].split(\".\")\n",
    "        filename = split_by_filename[0]\n",
    "        \n",
    "        if os.path.isfile(\"/hdd/datasets/Moments_in_Time_Mini/validation-sound/\" + action + \"/\" + split_by_action[1]):\n",
    "            partition['validation'].append(filename)\n",
    "            labels[filename] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(partition['train'])\n",
    "#print(partition['validation'])\n",
    "#print(labels.values())\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = MomentsDataset(partition['train'], labels)\n",
    "test_dataset = MomentsDataset(partition['validation'], labels)\n",
    "\n",
    "# DataLoader takes in:\n",
    "# batch_size - represents number of samples contained in each generated batch\n",
    "# shuffle - if true, denotes that we get a new order of exploration at each pass\n",
    "#           if false, denotes a linear exploration scheme\n",
    "# num_workers - denotes the number of processes that generate batches in parallel\n",
    "#               high number of workers means that CPU computations are efficiently managed\n",
    "train_loader = DataLoader(train_dataset, batch_size=args[\"batch-size\"], shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args[\"batch-size\"], shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The encoder generates a single output vector that embodies the input sequence meaning.\n",
    "    The general procedure is as follows:\n",
    "        1. In each step, a word will be fed to a network and it generates\n",
    "         an output and a hidden state.\n",
    "        2. For the next step, the hidden step and the next word will\n",
    "         be fed to the same network (W) for updating the weights.\n",
    "        3. In the end, the last output will be the representative of the input sentence (called the \"context vector\").\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, input_size, batch_size, num_layers=1, bidirectional=False):\n",
    "        \"\"\"\n",
    "        * For nn.LSTM, same input_size & hidden_size is chosen.\n",
    "        :param input_size: The size of the input vocabulary\n",
    "        :param hidden_size: The hidden size of the RNN.\n",
    "        :param batch_size: The batch_size for mini-batch optimization.\n",
    "        :param num_layers: Number of RNN layers. Default: 1\n",
    "        :param bidirectional: If the encoder is a bi-directional LSTM. Default: False\n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # The input should be transformed to a vector that can be fed to the network.\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim=hidden_size)\n",
    "\n",
    "        # The LSTM layer for the input\n",
    "        #self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        if args.bidirectional:\n",
    "            self.lstm_forward = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "            self.lstm_backward = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        if args.bidirectional:\n",
    "            input_forward, input_backward = input\n",
    "            hidden_forward, hidden_backward = hidden\n",
    "            input_forward = self.embedding(input_forward).view(1, 1, -1)\n",
    "            input_backward = self.embedding(input_backward).view(1, 1, -1)\n",
    "\n",
    "            out_forward, (h_n_forward, c_n_forward) = self.lstm_forward(input_forward, hidden_forward)\n",
    "            out_backward, (h_n_backward, c_n_backward) = self.lstm_backward(input_backward, hidden_backward)\n",
    "\n",
    "            forward_state = (h_n_forward, c_n_forward)\n",
    "            backward_state = (h_n_backward, c_n_backward)\n",
    "            output_state = (forward_state, backward_state)\n",
    "\n",
    "            return output_state\n",
    "        else:\n",
    "            # Make the data in the correct format as the RNN input.\n",
    "            embedded = self.embedding(input).view(1, 1, -1)\n",
    "            rnn_input = embedded\n",
    "            # The following descriptions of shapes and tensors are extracted from the official Pytorch documentation:\n",
    "            # output-shape: (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM\n",
    "            # h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state\n",
    "            # c_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state\n",
    "            output, (h_n, c_n) = self.lstm(rnn_input, hidden)\n",
    "            return output, (h_n, c_n)\n",
    "\n",
    "    def initHidden(self):\n",
    "\n",
    "        if self.bidirectional:\n",
    "            encoder_state = [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                                      torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]\n",
    "            encoder_state = {\"forward\": encoder_state, \"backward\": encoder_state}\n",
    "            return encoder_state\n",
    "        else:\n",
    "            encoder_state = [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                              torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]\n",
    "            return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This context vector, generated by the encoder, will be used as the initial hidden state of the decoder.\n",
    "    Decoding is as follows:\n",
    "    1. At each step, an input token and a hidden state is fed to the decoder.\n",
    "        * The initial input token is the <SOS>.\n",
    "        * The first hidden state is the context vector generated by the encoder (the encoder's\n",
    "    last hidden state).\n",
    "    2. The first output, shout be the first sentence of the output and so on.\n",
    "    3. The output token generation ends with <EOS> being generated or the predefined max_length of the output sentence.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, batch_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output, (h_n, c_n) = self.lstm(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, (h_n, c_n)\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"\n",
    "        The specific type of the hidden layer for the RNN type that is used (LSTM).\n",
    "        :return: All zero hidden state.\n",
    "        \"\"\"\n",
    "        return [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    This context vector, generated by the encoder, will be used as the initial hidden state of the decoder.\n",
    "    In case that their dimension is not matched, a linear layer should be used to transformed the context vector\n",
    "    to a suitable input (shape-wise) for the decoder cell state (including the memory(Cn) and hidden(hn) states).\n",
    "    The shape mismatch is True in the following conditions:\n",
    "    1. The hidden sizes of encoder and decoder are the same BUT we have a bidirectional LSTM as the Encoder.\n",
    "    2. The hidden sizes of encoder and decoder are NOT same.\n",
    "    3. ETC?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bidirectional, hidden_size_encoder, hidden_size_decoder):\n",
    "        super(Linear, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = int(bidirectional) + 1\n",
    "        self.linear_connection_op = nn.Linear(num_directions * hidden_size_encoder, hidden_size_decoder)\n",
    "        self.connection_possibility_status = num_directions * hidden_size_encoder == hidden_size_decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if self.connection_possibility_status:\n",
    "            return input\n",
    "        else:\n",
    "            return self.linear_connection_op(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
